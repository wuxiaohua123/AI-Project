{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ⅰ.动态网页爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "功能：逆向爬取网页--图片\n",
    "作者：吴小华\n",
    "时间：2019-7-22 上午\n",
    "'''\n",
    "import  requests\n",
    "import json\n",
    "\n",
    "#1.读取本地文件为json格式\n",
    "with open(r'C:\\Users\\苏哥哥\\Desktop\\111\\pic.txt','r') as pf:\n",
    "    pic_list = json.load(pf)\n",
    "print(len(pic_list))\n",
    "\n",
    "#2.解析出图片url链接\n",
    "pic_url = [] #定义一个空列表，用来存放图片url\n",
    "for i in range(len(pic_list)): #提取所有图片url\n",
    "   pic_url.append(pic_list[i]['cover'])\n",
    "print(pic_url)\n",
    "\n",
    "#3.根据url链接爬取图片到本地\n",
    "for j in range(len(pic_url)):\n",
    "    pic_html = requests.get(pic_url[j]) #发送图片的url进行请求\n",
    "    with open(r'C:\\Users\\苏哥哥\\Desktop\\111\\pic_%d.png'%j,'wb') as pf: #把图片保存到本地，以二进制形式写\n",
    "        pf.write(pic_html.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "功能：BeautifulSoup解析网页\n",
    "作者：吴小华\n",
    "时间：2019-7-22 上午\n",
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import chardet\n",
    "\n",
    "#1.发url请求\n",
    "url = 'http://www.163.com'\n",
    "ua = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1'}\n",
    "resp = requests.get(url,headers = ua,timeout=3)\n",
    "print(resp.encoding)\n",
    "\n",
    "#2.转换编码\n",
    "resp.encoding = chardet.detect(resp.content)['encoding']\n",
    "print(resp.encoding)\n",
    "\n",
    "#3.找到主网页所有文本和超链接url\n",
    "html = resp.content\n",
    "soup = BeautifulSoup(html,'html.parser') #创建bs对象\n",
    "soup.prettify #初始化对象\n",
    "class_data = soup.select('ul.cm_ul_round > li > a') #返回的是tag对象的列表\n",
    "child_title = []\n",
    "child_href = []\n",
    "for i in range(len(class_data)):\n",
    "    child_title.append(class_data[i].get_text()) #取其中的1个tag进行提取文本信息\n",
    "    child_href.append(class_data[i].get('href')) #取其中的1个tag进行提取超链接信息\n",
    "print(child_title)\n",
    "print(child_href)\n",
    "\n",
    "#4.爬取子网页url信息\n",
    "for i in range(10):\n",
    "    child_page = requests.get(child_href[i])\n",
    "    child_html = child_page.content\n",
    "    child_soup = BeautifulSoup(child_html,'html.parser') #创建bs对象\n",
    "    child_soup.prettify #初始化对象\n",
    "\n",
    "    child_text = child_soup.select('div.post_text > p')\n",
    "    child_text_list = []\n",
    "    for j in range(len(child_text)):\n",
    "        child_text_list.append(child_text[j].get_text())\n",
    "    \n",
    "    with open(r'C:\\Users\\苏哥哥\\Desktop\\111\\%d.txt'%i,'w') as pf:\n",
    "        for k in child_text_list:\n",
    "            pf.write(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ⅱ.爬取流浪地球"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "功能：爬取流浪地球\n",
    "作者：吴小华\n",
    "时间：2019-7-22 下午\n",
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#解析url函数\n",
    "def resp_def(url,ua):\n",
    "    resp = requests.get(url,headers=ua,timeout = 30)\n",
    "    return resp\n",
    "\n",
    "#转换为bs4格式函数\n",
    "def soup_def(resp):\n",
    "    #print(resp.encoding)\n",
    "    html = resp.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    soup.prettify()\n",
    "    return soup\n",
    "\n",
    "#解析元素函数\n",
    "def info_def(soup):\n",
    "    title1 = soup.select('title')\n",
    "    print(title1[0].get_text())\n",
    "    #提取名字\n",
    "    info_name = soup.select('header.main-hd > a.name')\n",
    "    #提取影评\n",
    "    info_yp = soup.select('header.main-hd > span')\n",
    "    #提取短频内容\n",
    "    info_cont = soup.select('div.short-content')\n",
    "\n",
    "    #提取url\n",
    "    info_url = soup.select('header.main-hd > a.avator')\n",
    "    name_list = []\n",
    "    title_list = []\n",
    "    time_list = []\n",
    "    cont_list = []\n",
    "    url_list = []\n",
    "    for i in range(len(info_name)):\n",
    "        name_list.append(info_name[i].get_text())\n",
    "        #print(name_list)\n",
    "        title_list.append(info_yp[i*2].get('title'))\n",
    "        #time_list.append(info_yp[i*2+1].get_text())\n",
    "        cont_list.append(info_cont[i].get_text())\n",
    "        url_list.append(info_url[i].get('href'))\n",
    "    return url_list\n",
    "\n",
    "#解析子网页函数\n",
    "def child_def(url_list):\n",
    "    address_list = []\n",
    "    #进入子网页爬取信息\n",
    "    for j in range(len(url_list)):  \n",
    "        child_html = resp_def(url_list[j],ua)\n",
    "        soup = soup_def(child_html)\n",
    "        #html = child_html.content\n",
    "        #soup = BeautifulSoup(html,'html.parser')\n",
    "        #soup.prettify()\n",
    "        info_child = soup.select('div.user-info > a')\n",
    "        if info_child == []:        \n",
    "            address_list.append('0')\n",
    "        else:\n",
    "            address_list.append(info_child[0].get_text())\n",
    "        #print(info_child)\n",
    "    print(address_list)\n",
    "#print(len(title_list))\n",
    "#print(len(time_list))\n",
    "\n",
    "#主函数\n",
    "if  __name__  == '__main__':\n",
    "    for i in range(4):\n",
    "        url = 'https://movie.douban.com/subject/26266893/reviews?start=%d'%(i*20)\n",
    "        ua = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1'}\n",
    "\n",
    "        url_list = info_def(soup_def(resp_def(url,ua)))\n",
    "        child_def(url_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
